{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def image_to_tensor(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 512)),  # Resize for faster processing\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.clone().detach().cpu().squeeze(0)\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    tensor = tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return np.array(transforms.ToPILImage()(tensor))\n",
    "\n",
    "# Content and Style Loss\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.loss = F.mse_loss(x, self.target)\n",
    "        return x\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = self.gram_matrix(target_feature).detach()\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        features = input.view(c, h * w)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(c * h * w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        G = self.gram_matrix(x)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_model_and_losses(cnn, style_img, content_img):\n",
    "    cnn = cnn.to(device).eval()\n",
    "    content_layers = ['conv_4']\n",
    "    style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "    model = nn.Sequential()\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    i = 0  # increment every time a conv is added\n",
    "\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim the model after last loss layer\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    model = model[:i + 1]\n",
    "\n",
    "    return model, style_losses, content_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, content_img, style_img, num_steps=200, style_weight=1e6, content_weight=1):\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img)\n",
    "    input_img = content_img.clone()\n",
    "\n",
    "    optimizer = torch.optim.LBFGS([input_img.requires_grad_()])\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = sum(sl.loss for sl in style_losses)\n",
    "            content_score = sum(cl.loss for cl in content_losses)\n",
    "\n",
    "            loss = style_score * style_weight + content_score * content_weight\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "results_nst = []\n",
    "\n",
    "print(\"Applying Neural Style Transfer...\")\n",
    "\n",
    "for i in range(min(len(cityscapes_images), len(dark_zurich_images))):\n",
    "    content_img = image_to_tensor(cityscapes_images[i])\n",
    "    style_img = image_to_tensor(dark_zurich_images[i])\n",
    "    output_nst_c2d = run_style_transfer(cnn, content_img, style_img)\n",
    "\n",
    "    content_img = image_to_tensor(dark_zurich_images[i])\n",
    "    style_img = image_to_tensor(cityscapes_images[i])\n",
    "    output_nst_d2c = run_style_transfer(cnn, content_img, style_img)\n",
    "\n",
    "    output_img_c2d = tensor_to_image(output_nst_c2d)\n",
    "    output_img_d2c = tensor_to_image(output_nst_d2c)\n",
    "\n",
    "    results_nst.append((cityscapes_images[i], dark_zurich_images[i], output_img_c2d, output_img_d2c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (cityscapes_img, dark_img, nst_c2d, nst_d2c) in enumerate(results_nst):\n",
    "    images = [cityscapes_img, dark_img, nst_c2d, nst_d2c]\n",
    "    titles = [\"Cityscapes\", \"Dark Zurich\", \"NST: Cityscapes→Dark\", \"NST: Dark→Cityscapes\"]\n",
    "\n",
    "    fig = visualize_images(images, titles, rows=2, cols=2, figsize=(14, 10))\n",
    "    plt.savefig(f\"nst_result_{i+1}.png\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"NST completed. Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
